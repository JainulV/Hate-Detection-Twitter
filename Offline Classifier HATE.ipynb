{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"./data/hate.csv\", delimiter=\",\", dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=X.iloc[:,6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.693255\n",
      "iteration 100: loss 0.238105\n",
      "iteration 200: loss 0.238084\n",
      "iteration 300: loss 0.238066\n",
      "iteration 400: loss 0.238050\n",
      "iteration 500: loss 0.238035\n",
      "iteration 600: loss 0.238019\n",
      "iteration 700: loss 0.238000\n",
      "iteration 800: loss 0.237972\n",
      "iteration 900: loss 0.237921\n",
      "iteration 1000: loss 0.237803\n",
      "iteration 1100: loss 0.237429\n",
      "iteration 1200: loss 0.235877\n",
      "iteration 1300: loss 0.227052\n",
      "iteration 1400: loss 0.214563\n",
      "iteration 1500: loss 0.206920\n",
      "iteration 1600: loss 0.202370\n",
      "iteration 1700: loss 0.199544\n",
      "iteration 1800: loss 0.197759\n",
      "iteration 1900: loss 0.196421\n",
      "iteration 2000: loss 0.195519\n",
      "iteration 2100: loss 0.194740\n",
      "iteration 2200: loss 0.194041\n",
      "iteration 2300: loss 0.193568\n",
      "iteration 2400: loss 0.193180\n",
      "iteration 2500: loss 0.192760\n",
      "iteration 2600: loss 0.192474\n",
      "iteration 2700: loss 0.192240\n",
      "iteration 2800: loss 0.192021\n",
      "iteration 2900: loss 0.191868\n",
      "iteration 3000: loss 0.191725\n",
      "iteration 3100: loss 0.191581\n",
      "iteration 3200: loss 0.191496\n",
      "iteration 3300: loss 0.191365\n",
      "iteration 3400: loss 0.191318\n",
      "iteration 3500: loss 0.191254\n",
      "iteration 3600: loss 0.191191\n",
      "iteration 3700: loss 0.190913\n",
      "iteration 3800: loss 0.191039\n",
      "iteration 3900: loss 0.190849\n",
      "iteration 4000: loss 0.190813\n",
      "iteration 4100: loss 0.190896\n",
      "iteration 4200: loss 0.190904\n",
      "iteration 4300: loss 0.190926\n",
      "iteration 4400: loss 0.190735\n",
      "iteration 4500: loss 0.190697\n",
      "iteration 4600: loss 0.190725\n",
      "iteration 4700: loss 0.190641\n",
      "iteration 4800: loss 0.190800\n",
      "iteration 4900: loss 0.190629\n"
     ]
    }
   ],
   "source": [
    "def train(X, y):\n",
    "    D = 50\n",
    "    K = 2\n",
    "    # initialize parameters randomly\n",
    "    h = 50 # size of hidden layer\n",
    "    W0 = 0.01 * np.random.randn(300,50)\n",
    "    b0 = np.zeros((1,50))\n",
    "    W = 0.01 * np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "    W2 = 0.01 * np.random.randn(h,K)\n",
    "    b2 = np.zeros((1,K))\n",
    "\n",
    "    # some hyperparameters\n",
    "    step_size = 0.825\n",
    "    reg = 1e-3 # regularization strength\n",
    "    \n",
    "    num_examples = len(y)\n",
    "    # pooling loop\n",
    "    kt = np.empty((len(y), 300))\n",
    "    for i in range(len(y)):\n",
    "        # word embeddings -> Max pooling / Average pooling -> 1-layer MLP -> ReLu -> Fully connected softmax\n",
    "        # word embed\n",
    "        Xi = nlp(X[i].decode('utf-8'))\n",
    "        t = len(Xi)\n",
    "        wt = map(lambda x: x.vector, Xi)\n",
    "        # ReLu\n",
    "        zt = np.maximum(0, wt)\n",
    "        # max pooling / Average pooling\n",
    "        #mt = np.max(zt, axis=0)\n",
    "        at = np.mean(zt, axis=0)\n",
    "        # concat\n",
    "        kt[i, :] = at #np.append(mt, at)\n",
    "        \n",
    "    # gradient descent loop\n",
    "    for j in xrange(5000): \n",
    "        # evaluate class scores, [N x K]\n",
    "        hidden_layer0 = np.dot(kt, W0) + b0\n",
    "        hidden_layer = np.maximum(0, np.dot(hidden_layer0, W) + b) # note, ReLU activation\n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "        data_loss = np.sum(correct_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        if j % 100 == 0:\n",
    "            print \"iteration %d: loss %f\" % (j, loss)\n",
    "            \n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y] -= 1\n",
    "        dscores /= num_examples\n",
    "  \n",
    "        # backpropate the gradient to the parameters\n",
    "        # first backprop into parameters W2 and b2\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "        # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[hidden_layer <= 0] = 0\n",
    "        # into W,b\n",
    "        dW = np.dot(hidden_layer0.T, dhidden)\n",
    "        db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "        # next backprop into hidden0 layer\n",
    "        dhidden0 = np.dot(dhidden, W.T)\n",
    "        # finally into W0, b0\n",
    "        dW0 = np.dot(kt.T, dhidden0)\n",
    "        db0 = np.sum(dhidden0, axis=0, keepdims=True)\n",
    "\n",
    "        # add regularization gradient contribution\n",
    "        dW2 += reg * W2\n",
    "        dW += reg * W\n",
    "        dW0 += reg * W0\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        W0 += -step_size * dW0\n",
    "        b0 += -step_size * db0\n",
    "    return b0, W0, b, W, b2, W2\n",
    "\n",
    "b=X.iloc[:,6].values\n",
    "a=map(int, X.iloc[:,5].values)\n",
    "a = map(lambda x : 1 if x == 0 else 0, a)\n",
    "l,m,p,q,r,s = train(b[:17000], a[:17000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.94\n",
      "f1-score:  0.9126919751220812\n",
      "auc:  0.8620982798081546\n",
      "accuracy: 0.96\n",
      "f1-score:  0.9395088725475187\n",
      "auc:  0.8215338401921093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "def predict(X, y, b0, W0, b, W, b2, W2):\n",
    "    kt = np.empty((len(y), 300))\n",
    "    for i in range(len(y)):\n",
    "        # word embeddings -> Max pooling / Average pooling -> 2-layer MLP -> ReLu -> Fully connected softmax\n",
    "        # word embed\n",
    "        Xi = nlp(X[i].decode('utf-8'))\n",
    "        t = len(Xi)\n",
    "        wt = map(lambda x: x.vector, Xi)\n",
    "        # ReLu\n",
    "        zt = np.maximum(0, wt)  #wt * (wt > 0)\n",
    "        # max pooling / Average pooling\n",
    "        #mt = np.max(zt, axis=0)\n",
    "        at = np.mean(zt, axis=0)\n",
    "        # concat\n",
    "        kt[i, :] = at #np.append(mt, at)\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer0 = np.dot(kt, W0) + b0\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer = np.maximum(0, np.dot(hidden_layer0, W) + b) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    print 'accuracy: %.2f' % (np.mean(predicted_class == y))\n",
    "    print 'f1-score: ', f1_score( y, predicted_class, average='weighted' )\n",
    "    print 'auc: ', roc_auc_score(y, scores[:, 1])\n",
    "    \n",
    "predict(b[:17000], a[:17000], l, m, p, q, r, s)\n",
    "predict(b[17000:], a[17000:], l, m, p, q, r, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
