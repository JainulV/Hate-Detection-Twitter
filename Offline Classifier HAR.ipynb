{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"./data/Online Harassment Dataset/onlineHarassmentDataset.tdf\", delimiter=\"\\t\", dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@Grumpy_P_Sloth @deanesmay feminists argue for raising minimum wage cause a women's studies major needs the $15 an hour wage at her McJob.\",\n",
       "       '1948Army of England helped the Jews to occupy Palestine.After that the Jews killed thousands of Englishmen and returned to England as dogs1',\n",
       "       'Dutch Leader Says Europe to collapse In 6 Weeks thnks > @BillGates, @GeorgeSoros, & @FWD_us #AltRight #WhiteGenocide https: / /t.co /rl1mh82jda',\n",
       "       ...,\n",
       "       'You Fucking Nigger You Did It To Yourself You Fucking Nigger You Killed Yourself You Fucking Nigger Believe Its Happening You Fucking Nigger',\n",
       "       'Young white woman gunned down by black beast in NorCal #WhiteLivesMatter http: / /t.co /GdLGO3fsqQ',\n",
       "       'Your descendants, the Jews, will be doomed to descend from the throne of power,'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=X.iloc[:,2].values\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.693275\n",
      "iteration 100: loss 0.591795\n",
      "iteration 200: loss 0.591774\n",
      "iteration 300: loss 0.591757\n",
      "iteration 400: loss 0.591742\n",
      "iteration 500: loss 0.591728\n",
      "iteration 600: loss 0.591714\n",
      "iteration 700: loss 0.591693\n",
      "iteration 800: loss 0.591653\n",
      "iteration 900: loss 0.591518\n",
      "iteration 1000: loss 0.590566\n",
      "iteration 1100: loss 0.580139\n",
      "iteration 1200: loss 0.574482\n",
      "iteration 1300: loss 0.574599\n",
      "iteration 1400: loss 0.566634\n"
     ]
    }
   ],
   "source": [
    "def train(X, y):\n",
    "    D = 50\n",
    "    K = 2\n",
    "    # initialize parameters randomly\n",
    "    h = 50 # size of hidden layer\n",
    "    W0 = 0.01 * np.random.randn(300,50)\n",
    "    b0 = np.zeros((1,50))\n",
    "    W = 0.01 * np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "    W2 = 0.01 * np.random.randn(h,K)\n",
    "    b2 = np.zeros((1,K))\n",
    "\n",
    "    # some hyperparameters\n",
    "    step_size = 1\n",
    "    reg = 1e-3 # regularization strength\n",
    "    \n",
    "    num_examples = len(y)\n",
    "    # pooling loop\n",
    "    kt = np.empty((len(y), 300))\n",
    "    for i in range(len(y)):\n",
    "        # word embeddings -> Max pooling / Average pooling -> 1-layer MLP -> ReLu -> Fully connected softmax\n",
    "        # word embed\n",
    "        Xi = nlp(X[i].decode('latin-1'))\n",
    "        t = len(Xi)\n",
    "        wt = map(lambda x: x.vector, Xi)\n",
    "        # ReLu\n",
    "        zt = np.maximum(0, wt)\n",
    "        # max pooling / Average pooling\n",
    "        #mt = np.max(zt, axis=0)\n",
    "        at = np.mean(zt, axis=0)\n",
    "        # concat\n",
    "        kt[i, :] = at #np.append(mt, at)\n",
    "        \n",
    "    # gradient descent loop\n",
    "    for j in xrange(1500): \n",
    "        # evaluate class scores, [N x K]\n",
    "        hidden_layer0 = np.dot(kt, W0) + b0\n",
    "        hidden_layer = np.maximum(0, np.dot(hidden_layer0, W) + b) # note, ReLU activation\n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "        data_loss = np.sum(correct_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        if j % 100 == 0:\n",
    "            print \"iteration %d: loss %f\" % (j, loss)\n",
    "            \n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y] -= 1\n",
    "        dscores /= num_examples\n",
    "  \n",
    "        # backpropate the gradient to the parameters\n",
    "        # first backprop into parameters W2 and b2\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "        # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[hidden_layer <= 0] = 0\n",
    "        # into W,b\n",
    "        dW = np.dot(hidden_layer0.T, dhidden)\n",
    "        db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "        # next backprop into hidden0 layer\n",
    "        dhidden0 = np.dot(dhidden, W.T)\n",
    "        # finally into W0, b0\n",
    "        dW0 = np.dot(kt.T, dhidden0)\n",
    "        db0 = np.sum(dhidden0, axis=0, keepdims=True)\n",
    "\n",
    "        # add regularization gradient contribution\n",
    "        dW2 += reg * W2\n",
    "        dW += reg * W\n",
    "        dW0 += reg * W0\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        W0 += -step_size * dW0\n",
    "        b0 += -step_size * db0\n",
    "    return b0, W0, b, W, b2, W2\n",
    "\n",
    "b=X.iloc[:,2].values\n",
    "a = map(lambda x : 1 if x == 'H' else 0, X.iloc[:,1].values)\n",
    "l,m,p,q,r,s = train(b[:15000], a[:15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.73\n",
      "f1-score:  0.6531040059076577\n",
      "auc:  0.6873971424528384\n",
      "accuracy: 0.78\n",
      "f1-score:  0.7016671527443462\n",
      "auc:  0.4216302353898262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "def predict(X, y, b0, W0, b, W, b2, W2):\n",
    "    kt = np.empty((len(y), 300))\n",
    "    for i in range(len(y)):\n",
    "        # word embeddings -> Max pooling / Average pooling -> 2-layer MLP -> ReLu -> Fully connected softmax\n",
    "        # word embed\n",
    "        Xi = nlp(X[i].decode('latin-1'))\n",
    "        t = len(Xi)\n",
    "        wt = map(lambda x: x.vector, Xi)\n",
    "        # ReLu\n",
    "        zt = np.maximum(0, wt)  #wt * (wt > 0)\n",
    "        # max pooling / Average pooling\n",
    "        #mt = np.max(zt, axis=0)\n",
    "        at = np.mean(zt, axis=0)\n",
    "        # concat\n",
    "        kt[i, :] = at #np.append(mt, at)\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer0 = np.dot(kt, W0) + b0\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer = np.maximum(0, np.dot(hidden_layer0, W) + b) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    print 'accuracy: %.2f' % (np.mean(predicted_class == y))\n",
    "    print 'f1-score: ', f1_score( y, predicted_class, average='weighted' )\n",
    "    print 'auc: ', roc_auc_score(y, scores[:, 1])\n",
    "\n",
    "predict(b[:15000], a[:15000], l, m, p, q, r, s)\n",
    "predict(b[15000:], a[15000:], l, m, p, q, r, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
